---
title: |
  <center> Transforming Data into Success: </center>
  <center> Bellabeat Case Study </center>
author: ""  
date: <center> José Jaén Delgado </center>
output:
  html_document: default
  pdf_document: default
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message = FALSE}
library(tidyverse) ; library(e1071) ; library(ggthemes) ; library(sjPlot) ; library(reshape2)

```


### Summary

|   The present project consists in a complete **business break-down investigation** using a variety of **data-based analytical tools**. Visualizations, statistical summaries and conclusions will be presented in a comprehensive fashion, showing every step that was taken as to introduce the reader to my **data analysis mindset** and **problem-solving capacities**


## 1. Business Task
\  

|   I  will analyze smart device fitness data for Bellabeat, a high-tech manufacturer of health-focused products for women. The main objective is to give sound advice so that it can **improve its current business plan** by applying a **data-driven decision making** framework. More concretely, I will select a Bellabeat product upon which the findings of my analysis of trends in smart device usage will be applied.
\  

* The first step of my analysis is **describing the available data**, which will provide a general idea about the situation we are facing. 

* A **data-cleaning process** will follow, so as to avoid dealing with misleading information. 

* Then, I will present relevant **summary statistics** along with insightful **data visualizations**, for which a proper **data manipulation** exercise is essential. 

* Finally, a conclusion will be reached in the form of **data-informed recommendations** that specifically target Bellabeat’s current and potential customers. Throughout the analysis described above, **Spreadsheets, SQL and R programming** will be used. 

\  

## 2. Data Description
\  

|   To collect the data I resorted to *Kaggle*, one of the largest online Data Science communities. Among an extensive list of datasets, FitBit Fitness Tracker data seem appropriate for our analysis owing to the possibility of recognizing patterns in smart device usage by studying human temporal routine behavior. Said datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk in December 2016. There are 18 CSV files in total, each one offering imperative information, for instance: *minute-level output for physical activity, heart rate*, and *sleep monitoring*. 
## 3. Data Cleaning and Analysis setup
\  

|   For the sake of simplicity, I will centralize all of the data into a **Relational Database** that will be concocted using **PostgreSQL**. This will allow me to easily manage the entirety of the files and make relevant queries, as the CSV files can be transformed into tables which I will link by joining common attributes. Note that the server for the aforementioned Database is *localhost*, thus, Kaggle notebooks will not be able to retrieve SQL queries' insights. Nonetheless, it is fairly easy to figure out a simple workaround: the results of my queries will be exported to a CSV file, allowing me to display a glimpse of said queries with R.
\  

## 4. Getting to know Smart Device Users

### 4.1. Basic Information and Limitations of Data
\  

|   After creating the database, it is time for a first glance at the available data:  
\  


```{sql, eval = FALSE}
SELECT COUNT(DISTINCT id) AS total_ids FROM daily_activity; 
```

```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

activity <- read.csv("dailyActivity_merged.csv", header = T, sep = ",", dec = ".")
length(unique(activity$Id))
```


```{sql, eval = FALSE}
SELECT COUNT(DISTINCT id) AS sleep_ids FROM daily_sleep;
```

```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

sleep <- read.csv("sleepDay_merged.csv", header = T, sep = ",", dec = ".")
length(unique(sleep$Id))
```


```{sql, eval = FALSE}
SELECT COUNT(DISTINCT id) AS weight_ids FROM weight;
```

```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")
weight <- read.csv("weightLogInfo_merged.csv", header = T, sep = ",", dec = ".")
length(unique(weight$Id))
```

```{sql, eval = FALSE}
SELECT COUNT(DISTINCT activitydate) AS days  
FROM daily_activity;
```

```{r, echo = FALSE}
length(unique(activity$ActivityDate))
```


|   A quick SQL query shows that data come from **33 unique users** for a time dimension of **31 days**. Only 8 runners have submitted their weight and approximately 27% did not use smart devices during their daily sleep. In addition, there is **no data about users' age, sex and height**, which is imperative in order to **draw conclusions** and **carry out statistical inference**. The missing runners can be identified by the following queries: 
\  


```{sql, eval = FALSE}
SELECT DISTINCT id 
FROM daily_activity
WHERE id NOT IN (SELECT DISTINCT id FROM daily_sleep);
```

```{sql, eval = FALSE}
SELECT DISTINCT id 
FROM daily_activity
WHERE id NOT IN (SELECT DISTINCT id FROM weight);
```
|   Runers ID's are not displayed due to data privacy reasons, keeping this sensitive information for internal analysis. The **total days of smart devices usage by user** and **average number of days that all runners tracked their activity** are, respectively:
\  

```{sql, eval = FALSE}
SELECT 
COUNT(DISTINCT activitydate) AS usage_days
FROM daily_activity
GROUP BY id;
```

```{r, echo = FALSE}
days_per_user <- activity %>% 
    select(ActivityDate) %>% 
    group_by(activity$Id) %>% 
    summarize(days_per_user = length(unique(ActivityDate)))
   
as.vector(as.matrix(days_per_user)[, 2])
```


```{sql, eval = FALSE}
WITH temp_table AS (
SELECT COUNT(DISTINCT activitydate)
FROM daily_activity
GROUP BY id)
SELECT ROUND(AVG(usage_days), 2) AS average_days 
FROM temp_table;
```

```{r, echo = FALSE}
round(mean(as.vector(as.matrix(days_per_user)[, 2])), 2)
```
|   As the usage of FitBit trackers is high in the sample, the aforementioned issue about missing information of daily sleep and weight may be explained by a lack of appropriate app reminder messages rather than users refusing to record their actions. 


### 4.2. Types of runners
\  

|   Now let us **categorize** runners according to their activity. For this purpose I will create **three user types**: *beginners*, *intermediate users* and *pro runners*. The **metrics threshold rule** to be applied is that of average total distance: if a user runs on average less than 5 kilometers it will fall in the first group. Should its average distance lies between 5 kilometers and 8 kilometers, it will be assigned to the intermediate class. Lastly, a runner is considered to be experienced if its mean distance is greater than 8 kilometers. The thresholds above have been chosen taking into account **total average distance**, **minimum average distance** and **maximum average distance** of the most dedicated runner:
\  


```{sql, eval = FALSE}
WITH temp_table2 AS (
SELECT 
    ROUND(AVG(TotalDistance), 2) AS mean_distance,
    COUNT(DISTINCT activitydate) AS number_of_days,
    id,
    CASE
        WHEN ROUND(AVG(TotalDistance), 2) < 5
        THEN 'Beginner Level'
        WHEN ROUND(AVG(TotalDistance), 2) >= 5 AND ROUND(AVG(TotalDistance), 2) < 8
        THEN 'Intermediate Level'
    ELSE 'Pro level'
    END AS user_type
FROM daily_activity
GROUP BY id)
SELECT 
  MAX(TEMP.mean_distance) AS average_distance_best_runner
  MIN(ACT.totaldistance) AS minimum_distance
  ROUND(AVG(ACT.totaldistance), 2) AS average_distance_all_runners
FROM temp_table2 TEMP
INNER JOIN daily_activity ACT ON TEMP.id = ACT.id;
```

```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

threshold_query <- read.csv("SQL_query_summary.csv", header = T, sep = ",", dec = ".")

matrix_query <- as.matrix(threshold_query)

colnames(matrix_query) <- c("Average best runner", "Minimum average distance", "Average Distance of All Users")

matrix_query
```

| Having expounded the reasoning behind the present threshold rule, we proceed with our analysis:
\  

```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

sql_query_user_types <- read.csv("SQL_Query_1.csv", header = T, sep = ",", dec = ".")

```

```{r}
## Setting up the data frame I will work with

average_by_user_data <- activity %>% 
    select(Id, TotalDistance) %>% 
    group_by(Id) %>% 
    summarize(avgerage_distance = mean(TotalDistance)) %>% 
    arrange(-avgerage_distance)

## Transforming the data frame above into a matrix so I can extract the average distance column

average_by_user_data <- as.matrix(average_by_user_data)
average_by_user <- as.vector(average_by_user_data[, 2])

## Algorithm to categorize users based on the metrics threshold rule

categorize_users <- function(x) {
    i <- 0 
    runner_type <- c(1:length(x))
    while (i < length(x)) {
        i <- i + 1
        if (x[i] < 5) {
            runner_type[i] <- "Beginner Level"
        } else if (x[i] >= 5 & x[i] < 8) {
            runner_type[i] <- "Intermediate Level"
        } else {
            runner_type[i] <- "Pro Level"
        }
    }
    return(runner_type)
}

## Adding a column with the user types

average_by_user_data <- cbind(average_by_user_data, categorize_users(average_by_user))
colnames(average_by_user_data) <- c("Id", "average_distance", "user_type")

## Removing Id's for privacy

average_by_user_data <- average_by_user_data[, colnames(average_by_user_data) != "Id"]

## Showing average distance and user type per runner

average_by_user_data
```

|   Being of considerable interest to know the **internal composition** of these groups, I will write an **algorithm** that counts the total number of runners in each group and then **plot a pie chart**. 
\  


```{r}
## User counting algorithm

count_user_types <- function(x) {
    i <- 0 ; intermediate <- 0
    beginner <- 0 ; pro <- 0
    while (i < length(x)) {
        i <- i + 1
        if (x[i] < 5) {
            beginner <- beginner + 1
        } else if (x[i] >= 5 & x[i] < 8) {
            intermediate <- intermediate + 1
        } else {
            pro <- pro + 1
        }
    }
    return(paste("Beginners: ", beginner, ", Intermediate: ", intermediate, ", Pro users: ", pro))
}

## Group composition

count_user_types(as.numeric(average_by_user_data[,1]))
```

```{r, echo = FALSE}
count_user_types <- function(x) {
    intermediate <- 0
    beginner <- 0 ; pro <- 0
    for (i in 1:length(x)) {
        if (x[i] < 5) {
            beginner <- beginner + 1
        } else if (x[i] >= 5 & x[i] < 8) {
            intermediate <- intermediate + 1
        } else {
            pro <- pro + 1
        }
    }
    return(c(beginner, intermediate, pro))
}
```



```{r}
## Setting up the pie chart with categories and number of users by type

user_types <- c("Pro Level", "Intermediate Level", "Beginner Level")

group_composition <- c(count_user_types(as.numeric(average_by_user_data[,1]))[3], 
           count_user_types(as.numeric(average_by_user_data[,1]))[2], 
           count_user_types(as.numeric(average_by_user_data[,1]))[1])

## Creating a data frame with the information above

datafr <- data.frame(user_types, group_composition)

## Calculating the relevant values that will make up the plot

data <- datafr %>% 
    arrange(desc(user_types)) %>% 
    mutate(prop = group_composition / sum(group_composition) * 100) %>% 
    mutate(ypos = cumsum(prop) - 0.5 * prop)
    
pie_chart <- ggplot(data = data, aes(x = 2, y = prop, fill = user_types)) +
  geom_bar(stat = "identity", color = "white") +
  coord_polar("y", start = 0) + theme_void() + 
  geom_text(aes(y = ypos, label = round(prop, 2)), color = "white", size = 6) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "User type Pie Chart", subtitle = "Expressed as percentage") + xlim(0.5, 2.5)

```
<center>
```{r, echo = FALSE, message = FALSE}
pie_chart
```
</center>


|   It can be observed that whereas **84.84%** of total users are either **getting started to running or moderately doing the sport**, the rest have taken it to another level by enduring long distance running.
| We could also dig deep through the data by studying the **distribution of running distance per user type**, which certainly enriches the statistical aspect of the project as it would allow to **compute the probability of running a certain distance given a user type**. 
In order to graphically show the distribution of running distance, we perform a SQL query first, displaying essential data for our histogram. We then export such information to a CSV file and read it with R:

```{sql, eval = FALSE}
WITH temp_table3 AS (
SELECT 
    CASE
        WHEN ROUND(AVG(TotalDistance), 2) < 5
        THEN 'Beginner Level'
        WHEN ROUND(AVG(TotalDistance), 2) >= 5 AND 
        ROUND(AVG(TotalDistance), 2) < 8
        THEN 'Intermediate Level'
    ELSE 'Pro level'
    END AS user_type,
    id
FROM daily_activity
GROUP BY id)
SELECT 
    ACT.*, TEMPO.* 
FROM daily_activity ACT
INNER JOIN temp_table3 TEMPO ON TEMPO.id = ACT.id;
```
    
```{r, echo = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

query3 <- read.csv("SQL_Query_3.csv", header = T, sep = ",", dec = ".")

attach(query3)
```
```{r, message = FALSE}
distribution <- ggplot(data = query3, aes(x = totaldistance, color = user_type, fill = user_type)) +
geom_histogram(aes(y = ..density..), position = "identity", alpha = 0.5) + 
geom_density(alpha = 0.6) +
geom_vline(data = query3, aes(xintercept = mean(totaldistance), color = user_type), linetype = "dashed") +
scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9")) +
scale_fill_manual(values = c("#999999", "#E69F00", "#56B4E9")) +
labs(title = "Running Distance Distribution", x = "Distance (km)", y = "Density") + theme_economist() +
annotate("text", x = 10, y = 0.21, label = "Average: 5.49 km", color = "#FE7F9C", fontface = "bold", size = 4.5) +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))
```
<center>
```{r, echo = FALSE, message = FALSE}
distribution
```
</center>

\  

|   As an example, let us calculate the probability of running less than the average total distance for each user type. Note that the probability density function of a continuous variable is defined as

<center>
\begin{equation*}
Pr(a \leq X \leq b) = \int_{a}^{b} f_x(x)  \,dx 
\end{equation*}
</center>
\  

|   Particularizing for our case, we can approximate said expression with the estimator below as the distance variabe is discretized:
<center>
\begin{equation*}
\frac{\sum\limits_{i = 1}^{N}X_{i,g}}{\sum\limits_{i = 1}^{N}X_{i,g} + \sum\limits_{i = 1}^{N}Y_{i,g}}  
\end{equation*}
</center>

\  

|   Where:
\  


* N = Total number of runs

* i = Individual *i*

* g = User type (beginner, intermediate or pro)

* X = Binary variable taking value 1 if a certain run of individual *i* pertaining to group *g* exceeds 5.49 km and 0 otherwise

* Y = Counterpart of X (dummy values are swapped)

\  

```{r}
## Probability is calculated as the number of times a user ran less than 5.49 km with respect to its total runs

prob_pro <- length(query3$totaldistance[query3$user_type == "Pro level" & query3$totaldistance < 5.49])/length(query3$totaldistance[query3$user_type == "Pro level"])
prob_mid <- length(query3$totaldistance[query3$user_type == "Intermediate Level" & query3$totaldistance < 5.49])/length(query3$totaldistance[query3$user_type == "Intermediate Level"])
prob_noob <- length(query3$totaldistance[query3$user_type == "Beginner Level" & query3$totaldistance < 5.49])/length(query3$totaldistance[query3$user_type == "Beginner Level"])
round(c(prob_noob, prob_mid, prob_pro),2)

```
|   It is no surprise to find that **86% of beginners' runs** do not overcome 5.49 kilometers, while **62% of intermediate-level runs** do. Experienced users are pretty unlikely to record a run whose distance is under the average (only 12%).


\  

## 4.3 Intensity & Calories

|   Moving on to other variables, the **relationship between intensity and number of burnt calories** is of considerable importance. Let us gradually define the former as *sedentary*, *fair*, *light* and *active*. Continuing with the user categorization of section 4.2, let's take a closer look at how these variables correlate conditional on the runner's type. Firstly we set up the data frame by making a SQL query and then we resort to R.
```{sql, eval = FALSE}
SELECT 
    sedentaryminutes AS sedentary_minutes,
    lightlyactiveminutes AS lightly_minutes,
    fairlyactiveminutes AS fairly_active_minutes,
    veryactiveminutes AS very_active_minutes,
    calories, 
     CASE
        WHEN ROUND(AVG(TotalDistance), 2) < 5
        THEN 'Beginner Level'
        WHEN ROUND(AVG(TotalDistance), 2) >= 5 AND 
        ROUND(AVG(TotalDistance), 2) < 8
        THEN 'Intermediate Level'
    ELSE 'Pro level'
    END AS user_type
FROM daily_activity
GROUP BY 
  id, sedentaryminutes, lightlyactiveminutes, 
  fairlyactiveminutes, veryactiveminutes, calories;
```

```{r, echo = FALSE, message = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

query5 <- read.csv("SQL_Query_5.csv")

attach(query5)
```

```{r, message = FALSE}
sedentary <- ggplot(data = query5, aes(y = calories, x = sedentary_minutes, color = user_type)) + geom_point() +
theme_economist() + geom_smooth(method = "lm") +
labs(title = "Calories vs Sedentary Intensity", x = "Sedentary Minutes", y = "Calories") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))

fair <- ggplot(data = query5, aes(y = calories, x = fairly_active_minutes, color = user_type)) + geom_point() +
theme_economist() + geom_smooth(method = "lm") +
labs(title = "Calories vs Fair Intensity", x = "Fairly Active Minutes", y = "Calories") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))

light <- ggplot(data = query5, aes(y = calories, x = lightly_minutes, color = user_type)) + geom_point() +
theme_economist() + geom_smooth(method = "lm") +
labs(title = "Calories vs Light Intensity", x = "Lightly Active Minutes", y = "Calories") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))

active <- ggplot(data = query5, aes(y = calories, x = very_active_minutes, color = user_type)) + geom_point() +
theme_economist() + geom_smooth(method = "lm") +
labs(title = "Calories vs Active Intensity", x = "Very Active Minutes", y = "Calories") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))
```
<center>
```{r, echo = FALSE, message = FALSE}
sedentary
fair
light
active
```
</center>
\  

|   It can be observed that **the higher the number of running minutes, the more calories individuals burn**. Advanced runners tend to come on top of the rest, which is expected. Let us now take a closer look at the **average activity intensity minutes by user type**:
    
```{sql, eval = FALSE}
SELECT  
    INTEN.id, 
    ROUND(AVG(INTEN.sedentaryminutes), 2) AS average_sedentary,
    ROUND(AVG(INTEN.lightlyactiveminutes), 2) AS average_light,
    ROUND(AVG(INTEN.fairlyactiveminutes), 2) AS average_faire,
    ROUND(AVG(INTEN.veryactiveminutes), 2) AS average_active, 
    CASE
        WHEN ROUND(AVG(ACT.TotalDistance), 2) < 5
        THEN 'Beginner Level'
        WHEN ROUND(AVG(ACT.TotalDistance), 2) >= 5 AND 
        ROUND(AVG(ACT.TotalDistance), 2) < 8
        THEN 'Intermediate Level'
    ELSE 'Pro level'
    END AS user_type
FROM daily_activity ACT
JOIN daily_intensities INTEN
ON INTEN.id = ACT.id
GROUP BY INTEN.id
ORDER BY ROUND(AVG(ACT.TotalDistance), 2) DESC;
```

```{r, echo = FALSE, message = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

query6 <- read.csv("SQL_Query_6.csv", header = T, sep = ",", dec = ".")

attach(query6)
```

```{r, message = FALSE}
## Changing the data format to 'long'

means.long <- melt(query6, id.vars = "user_type")

## Setting up our visualization

intensity_by_type <- ggplot(means.long, aes(x = variable, y = value, fill = user_type)) +
                          geom_bar(stat = "identity",position = "dodge") +
                          scale_fill_discrete(name = "User Type",
                          labels = c("Beginner", "Intermediate", "Pro")) +
                          scale_x_discrete(labels = c('Sedentary','Fair','Light', 'Active')) +
                          xlab("Intensity") + ylab("Number of minutes") + theme_economist() +
                          labs(title = "Average Intensity Minutes") +
                          theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))
```
<center>
```{r, echo = FALSE, message = FALSE}
intensity_by_type
```
</center>
\ 

|   **Sedentary minutes are ubiquitous across all groups**, standing out how poorly runners perform at intensity. However, it is not surprising that any individual does not devote all of their time to doing sport. Consequently, the distribution shape of time allocation should not take us aback as, among other variables, working, sleeping and commuting take up an important number of hours.


## 4.4 Sleep time

\  

|   We can study the relationship between sleeping and running performance. Firstly, let's analyze the **distribution of daily average hours of sleep**: 

\  

```{sql, eval = FALSE}
SELECT 
    TO_CHAR(TO_DATE(sleepday, 'MM/DD/YYYY'), 'Dy') AS dates,
    ROUND(totaltimeinbed/60, 2) AS bed_hours
FROM daily_sleep
GROUP BY dates, bed_hours
ORDER BY dates ASC;
```
\  

|   Exporting the resulting data frame to a CSV file and reading it with R the following **boxplot** is retrieved:
\  

```{r, echo = FALSE, message = FALSE, warning = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

boxplot_data <- read.csv("SQL_Query_8.csv", header = T, sep = ",", dec = ".")

attach(boxplot_data)
```
    
```{r, message = FALSE, warning = FALSE}
## Converting strings into factors so that we can manually order days

boxplot_data$dates <- factor(boxplot_data$dates, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))

## Boxplot 

boxplot <- ggplot(data = boxplot_data, aes(x = dates, y = bed_hours, fill= dates)) + 
    geom_boxplot(alpha = 0.8) +
    guides(fill=FALSE, color = FALSE) +
    theme(legend.position="none") + theme_economist() +
    labs(title = "Sleep hours", x = "", y = "Hours") +
    theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))
```
<center>
```{r, echo = FALSE, message = FALSE, warning = FALSE}
boxplot
```
</center>
\  

|   **Outliers in the lower tail of the distribution** are present throughout all days, especially on Thursdays, Fridays and Sundays. There is a general **upward trend as weekend approaches**, which gives users the opportunity to increase their sleep time. Focusing on user type analysis:
\  

```{r, echo = FALSE, message = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

relations <- read.csv("relations.csv", header = T, sep = ",", dec = ".")
```
```{sql, eval = FALSE}
SELECT 
    ROUND(SLEEP.totaltimeinbed/60, 2) AS sleep_hours,
    ROUND(ACT.totaldistance, 2) AS distance,
    ACT.activitydate AS date,
    CASE
        WHEN ROUND(AVG(ACT.TotalDistance), 2) < 5
        THEN 'Beginner Level'
        WHEN ROUND(AVG(ACT.TotalDistance), 2) >= 5 AND 
        ROUND(AVG(ACT.TotalDistance), 2) < 8
        THEN 'Intermediate Level'
    ELSE 'Pro level'
    END AS user_type
FROM daily_activity ACT
INNER JOIN daily_sleep SLEEP 
ON SLEEP.id = ACT.id
GROUP BY ACT.id, sleep_hours, distance, date
ORDER BY date ASC;
```

```{r, message = FALSE}
## Visualizing a violin plot

violin_plot <- ggplot(data = relations, aes(x = distance, y = sleep_hours, fill = user_type)) + 
geom_violin() + theme_economist() +
scale_fill_manual(values = c("#FE7F9C" , "#AF69EF" , "#A1045A")) +
scale_color_manual(values = c("#FE7F9C" , "#AF69EF" , "#A1045A")) +
labs(title = "Sleep hours vs Total Distance", x = "Distance (kms)", y = "Hours") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, vjust = -2))
```

<center>
```{r, echo = FALSE, message = FALSE, warning = FALSE}
violin_plot
```
</center>

\  

|   **No clear relationship between sleeping time and running distance** can be drawn from the violin plot. Beginners and intermediate runners seem to show a stable sleeping schedule, centered around their respective means. High-level users' distance is volatile, suggesting there may be different subcategories within said group.  

## 5. Regression Analysis

|   Leveraging the available data and taking into account that one of the top reasons people start running is the fast pace at which fat is lost, **it is possible to predict how many calories an individual will burn given certain characteristics**. In this section I propose a statistical way to attain such objective. Nevertheless, considering that we lack sleep information about some individuals, weight data is definitely scarce and we have no clue about users' age, sex and height, **Gauss-Markov assumptions are unlikely to hold**, specifically zero expectation of the error term conditional on the regressors: $E[\epsilon_i|X_1, X_2, ..., X_n] = 0$. Thus, **estimates will probably be biased**, namely: $E[\hat\beta_i] \neq \beta_i$   $\forall$ i that $cov(X_i,\epsilon_i) \neq 0$. The drawbacks noted before should not refrain us from proceeding with the regression analysis, as we are not so interested in obtaining a clear causal relationship between burnt calories and other variables but rather gain general insights.
\  

Let us estimate the following linear regression model with pooled OLS:
\  

<center>
$calories_{i,t }= \gamma_0 + \gamma_1pro_{i,t } + \gamma_2intermediate_{i,t } + \gamma_3active_{i,t } + \gamma_4light_{i,t } + \gamma_5sedentary_{i,t } + \gamma_6bed_{i,t } + \epsilon_{i,t }$
</center> 
\  

|   Where *pro* and *intermediate* are binary variables that take the value 1 if individual 'i' is from that user type. Notice that we drop *beginner* to avoid perfect multicollinearity (so $\hat\gamma_0$ captures the effect of being a starter). *Active*, *light* and *sedentary* stand for running intensity measured in minutes and *bed* is total time in bed.
\  

```{r, echo = FALSE, message = FALSE}
setwd("~/Desktop/Universidad/Otros/Courses/Google Data Analytics/Project/Data")

regression <- read.csv("regression.csv", header = T, sep = ",", dec = ".")
```

```{r}
## Constructing the model

model <- lm(formula = calories ~ pro + intermediate + veryactiveminutes + lightlyactiveminutes + sedentaryminutes + totaltimeinbed, data = regression)

## Getting Results

model_summary <- tab_model(model)
```
<center>
```{r, echo = FALSE, message = FALSE}
## Constructing the model
model_summary
```
</center>


|   As we pointed out before, no causal effect can be retrieved from our model. However, there are some relevant aspects:
\  


* Being an **experienced runner** is associated with burning, on average, **624.24 additional calories** in comparison with a beginner, ceteris paribus.

* Pertaining to the **intermediate-level group** also presents an advantage: on average they **burn 68.95 extra calories** than startes, ceteris paribus. 

* On average, each additional minute at a very **active intensity is associated with burning 14 calories**, ceteris paribus.

* Marginal effect of an additional **minute at light intensity is associated with burning 2.38 calories**, ceteris paribus.

* The average marginal effect of an additional **sedentary minute is associated with burning 0.5 calories**, keeping the rest of the variables constant.

* The effect of **bed time is not statistically significant** (consistent with the visual intuition presented in section 4.4).

\  


## 6. Recommendations
\  

Based in the data analysis I have provided my final recommendations are:

* Develop the Bellabeat app in such a way that **essential information** such as weight, height and age **is mandatory** in order to sign up.     
**How**: Algorithm with an 'input' command that asks for this data with a loop that repeats said questions in case they have been incorrectly answered.  
**Benefit**: Further specific user data to be used for an enhanced segmentation.


* Implement the **algorithm** that was proposed in section 4.2 to categorize users. **Specific messages** can target runners according to their intensity/distance level. For instance, if the user is a beginner, the app could give advice on proper breathing for running, best way to limber up and congratulating messages once a new distance record is attained. In the case of intermediate or high-level users, news about upcoming competitions, top-notch diets and challenges seem more appropriate than giving out general wisdom that they most certainly already know.    
**How**: Algorithm that updates user's status based on latest available data. Linking path to running news and blogs that serve as a guide to improve.  
**Benefit**: Motivation to train and as a consequence higher usage of Bellabeat's app.
    

* Remind users of the relationship between burning calories and activity intensity as well a total distance. Display an interactive dashboard with up-to-date running statistics to show historical performance.  
**How**: Add new variables to the linear regression model proposed in section 5 and run the 'predict' code in R to show predicted burnt calories if the user increased running intensity or opted for longer distances.  
**Benefit**: Provide useful information specifically targetted to runners, ultimately nudging clients to set new goals and assiduously use the app.


